{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿How well are the first task representations preserved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "seed = 1993\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_dir = \"/home/studio-lab-user/CIL_Survey/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Mean: ['0.5071', '0.4865', '0.4409']\n",
      "Std: ['0.2673', '0.2564', '0.2762']\n"
     ]
    }
   ],
   "source": [
    "train_dataset_gpu = {}\n",
    "eval_dataset_gpu = {}\n",
    "\n",
    "# dataset\n",
    "train = torchvision.datasets.CIFAR100(root=data_dir, download=True, transform=transforms.ToTensor())\n",
    "eval = torchvision.datasets.CIFAR100(root=data_dir, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# move dataset to gpu\n",
    "train_dataset_gpu_loader = torch.utils.data.DataLoader(train, batch_size=len(train), drop_last=True,\n",
    "                                            shuffle=True, num_workers=2, persistent_workers=False)\n",
    "eval_dataset_gpu_loader = torch.utils.data.DataLoader(eval, batch_size=len(eval), drop_last=True,\n",
    "                                            shuffle=False, num_workers=1, persistent_workers=False)\n",
    "train_dataset_gpu['images'], train_dataset_gpu['targets'] = [item.to(device=\"cuda\", non_blocking=True) for item in next(iter(train_dataset_gpu_loader))]\n",
    "eval_dataset_gpu['images'],  eval_dataset_gpu['targets']  = [item.to(device=\"cuda\", non_blocking=True) for item in next(iter(eval_dataset_gpu_loader)) ]\n",
    "\n",
    "# normalize images\n",
    "train_cifar_std, train_cifar_mean = torch.std_mean(train_dataset_gpu['images'], dim=(0, 2, 3)) \n",
    "print(f\"Mean: {[f'{x:.4f}' for x in train_cifar_mean.tolist()]}\")\n",
    "print(f\"Std: {[f'{x:.4f}' for x in train_cifar_std.tolist()]}\")\n",
    "def batch_normalize_images(input_images, mean, std):\n",
    "    return (input_images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n",
    "batch_normalize_images = partial(batch_normalize_images, mean=train_cifar_mean, std=train_cifar_std)\n",
    "train_dataset_gpu['images'] = batch_normalize_images(train_dataset_gpu['images'])\n",
    "eval_dataset_gpu['images']  = batch_normalize_images(eval_dataset_gpu['images'])\n",
    "\n",
    "data = {\n",
    "        'train': train_dataset_gpu,\n",
    "        'eval': eval_dataset_gpu\n",
    "    }\n",
    "\n",
    "# pad images for later random cropping\n",
    "pad_amount = 4\n",
    "data['train']['images'] = F.pad(data['train']['images'], (pad_amount,)*4, 'reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torchvision.models.resnet18()\n",
    "net.to(device);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_transforms import get_batches\n",
    "\n",
    "# training\n",
    "def train(optimizer, task):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    indices = range(increment * task, increment * (task + 1))\n",
    "    for batch_idx, (inputs, targets) in enumerate(get_batches(data, \"train\", batch_size, indices=indices)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets) if task == 0 else criterion(outputs[:, task * increment:], targets - task * increment)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += len(targets)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss/(batch_idx + 1), 100.*correct/total\n",
    "\n",
    "def eval(from_task, to_task):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    indices = range(from_task * increment, (to_task+1) * increment)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(get_batches(data, \"eval\", batch_size, indices=indices)):\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += len(targets)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        return test_loss/(batch_idx + 1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to train the linear classifier on a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(optimizer, task):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    indices = range(increment * task, increment * (task + 1))\n",
    "    # freeze all layers but the classifier\n",
    "    for n, p in net.named_parameters():\n",
    "        if n != \"fc.weight\" and n != \"fc.bias\":\n",
    "            p.requires_grad = False\n",
    "    for batch_idx, (inputs, targets) in enumerate(get_batches(data, \"train\", batch_size, indices=indices)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets) if task == 0 else criterion(outputs[:, task * incement:], targets - task * increment)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += len(targets)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    return train_loss/(batch_idx + 1), 100.*correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train ideal model on first task and get the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "increment = 5\n",
    "total_classes = 100 \n",
    "feature_dim = 512\n",
    "batch_size = 128\n",
    "\n",
    "# optimizer and scheduler config\n",
    "init_epochs = 200\n",
    "init_lr = 0.1\n",
    "init_weight_decay = 0.0005\n",
    "\n",
    "rest_epochs = 80\n",
    "rest_lr = 0.1\n",
    "rest_weight_decay = 2e-4\n",
    "rest_milestones = [40, 70]\n",
    "rest_lr_decay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_train(task, to_task, linear_probe=False):\n",
    "    # update linear classifier\n",
    "    new_fc = nn.Linear(feature_dim, increment * (task+1), device=\"cuda\")\n",
    "    nn.init.kaiming_uniform_(new_fc.weight, nonlinearity='linear')\n",
    "    nn.init.constant_(new_fc.bias, 0)\n",
    "    if linear_probe == False:\n",
    "        if task != 0:\n",
    "            old_fc = net.fc\n",
    "            new_fc.weight.data[:task * increment] = old_fc.weight\n",
    "            new_fc.bias.data[:task * increment] = old_fc.bias\n",
    "        net.fc = new_fc\n",
    "\n",
    "    # select optimize and scheduler\n",
    "    if task == 0:\n",
    "        epochs = init_epochs\n",
    "        optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=init_lr, weight_decay=init_weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    else: \n",
    "        epochs = rest_epochs\n",
    "        optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=rest_lr, weight_decay=rest_weight_decay)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=rest_milestones, gamma=rest_lr_decay)\n",
    "\n",
    "    pbar = tqdm(range(epochs), unit=\"epoch\")\n",
    "    for epoch in pbar:\n",
    "        tloss, tacc = train(optimizer, task) if linear_probe == False else train_classifier(optimizer, task)\n",
    "        eloss, eacc = eval(from_task=0, to_task=to_task)\n",
    "        scheduler.step()\n",
    "        # metrics \n",
    "        # if epoch == epochs - 1:\n",
    "        #     tlosses.append(tloss)\n",
    "        #     taccs.append(tacc)\n",
    "        #     elosses.append(eloss)\n",
    "        #     eaccs.append(eacc)\n",
    "        pbar.set_postfix({\"Train Loss\": tloss, \"Train Acc\": tacc, \"Eval Loss\": eloss, \"Eval Acc\": eacc})\n",
    "\n",
    "    return eloss, eacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:40<00:00,  1.99epoch/s, Train Loss=0.00422, Train Acc=99.9, Eval Loss=0.908, Eval Acc=82.8]\n"
     ]
    }
   ],
   "source": [
    "loss0, accuracy0 = incremental_train(task=0, linear_probe=False, to_task=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9079679250717163\n",
      "82.8125\n"
     ]
    }
   ],
   "source": [
    "print(loss0)\n",
    "print(accuracy0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train encoder on second task and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 80/80 [00:39<00:00,  2.01epoch/s, Train Loss=0.0167, Train Acc=97.2, Eval Loss=4.33, Eval Acc=4.95]\n"
     ]
    }
   ],
   "source": [
    "loss1, accuracy1 = incremental_train(task=1, linear_probe=False, to_task=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.334564685821533\n",
      "4.947916666666667\n"
     ]
    }
   ],
   "source": [
    "print(loss1)\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train linear classifier on first task and measure accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:21<00:00,  9.36epoch/s, Train Loss=0.822, Train Acc=68.5, Eval Loss=0.831, Eval Acc=68.5]\n"
     ]
    }
   ],
   "source": [
    "loss_lp, accuracy_lp = incremental_train(task=0, linear_probe=True, to_task=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.831021249294281\n",
      "68.48958333333333\n"
     ]
    }
   ],
   "source": [
    "print(loss_lp)\n",
    "print(accuracy_lp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the ideal accuracy with the degraded accuarcy with the linear probe accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the linear probe accuracy is close to the ideal accuracy, then the catasthrofical fogetting is caused beacuse a degradation on the linear classifier,\n",
    "If not, it is caused because of a degradation on the feature representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
