{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetuning strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment information**\n",
    "\n",
    "- dataset: cifar100\n",
    "- shuffle: True\n",
    "- init_cls: 5\n",
    "- increment: 5\n",
    "- convnet_type: resnet32\n",
    "- seed: 1993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sets seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seed = 1993\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from utils.data_manager import DataManager\n",
    "\n",
    "data_manager = DataManager(dataset_name=\"cifar100\", shuffle=True, seed=seed, init_cls=5, increment=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Maybe optimize transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Train data shape: (50000, 32, 32, 3)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "\n",
      "Train transforms: [RandomCrop(size=(32, 32), padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(brightness=(0.7529411764705882, 1.2470588235294118), contrast=None, saturation=None, hue=None)]\n",
      "Test transforms: []\n",
      "Common transforms: [ToTensor(), Normalize(mean=(0.5071, 0.4867, 0.4408), std=(0.2675, 0.2565, 0.2761))]\n"
     ]
    }
   ],
   "source": [
    "print(type(data_manager._train_data))\n",
    "print(\"Train data shape:\", data_manager._train_data.shape)\n",
    "print(\"Test data shape:\", data_manager._test_data.shape)\n",
    "\n",
    "print()\n",
    "print(\"Train transforms:\", data_manager._train_trsf)\n",
    "print(\"Test transforms:\", data_manager._test_trsf)\n",
    "print(\"Common transforms:\", data_manager._common_trsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model strategy       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import factory\n",
    "\n",
    "# model_args = {\"convnet_type\": \"resnet32\", \"skip\": False, \"memory_size\": 200, \"device\": [torch.device(\"cuda:0\"), torch.device(\"cuda:1\")]}\n",
    "model_args = {\"convnet_type\": \"conv2\", \"skip\": False, \"memory_size\": 200, \"device\": [torch.device(\"cuda:0\"), torch.device(\"cuda:1\")]}\n",
    "model = factory.get_model(\"finetune\", args=model_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncrementalNet(\n",
       "  (convnet): ConvNet2(\n",
       "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/torch/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data_manager\u001b[39m.\u001b[39mnb_tasks):\n\u001b[0;32m----> 2\u001b[0m     model\u001b[39m.\u001b[39;49mincremental_train(data_manager)\n\u001b[1;32m      3\u001b[0m     model\u001b[39m.\u001b[39meval_task(save_conf\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m     model\u001b[39m.\u001b[39mafter_task()\n",
      "File \u001b[0;32m~/CIL_Survey/models/finetune.py:67\u001b[0m, in \u001b[0;36mFinetune.incremental_train\u001b[0;34m(self, data_manager)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiple_gpus) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDataParallel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiple_gpus)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_loader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_loader)\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiple_gpus) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network\u001b[39m.\u001b[39mmodule\n",
      "File \u001b[0;32m~/CIL_Survey/models/finetune.py:72\u001b[0m, in \u001b[0;36mFinetune._train\u001b[0;34m(self, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_loader, test_loader):\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device)\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cur_task \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     74\u001b[0m         optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(\n\u001b[1;32m     75\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_network\u001b[39m.\u001b[39mparameters(),\n\u001b[1;32m     76\u001b[0m             momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m,\n\u001b[1;32m     77\u001b[0m             lr\u001b[39m=\u001b[39minit_lr,\n\u001b[1;32m     78\u001b[0m             weight_decay\u001b[39m=\u001b[39minit_weight_decay,\n\u001b[1;32m     79\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.11/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    299\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "for task in range(data_manager.nb_tasks):\n",
    "    model.incremental_train(data_manager)\n",
    "    model.eval_task(save_conf=False)\n",
    "    model.after_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 2500\n",
      "Length of datalaoder: 20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "model._network.update_fc(5)\n",
    "model._total_classes = 5\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "\n",
    "# setup datasets and dataloaders\n",
    "train_dataset = data_manager.get_dataset(\n",
    "    np.arange(model._known_classes, model._total_classes),\n",
    "    source=\"train\",\n",
    "    mode=\"train\",\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, persistent_workers=True, pin_memory=True\n",
    ")\n",
    "test_dataset = data_manager.get_dataset(\n",
    "    np.arange(0, model._total_classes), source=\"test\", mode=\"test\"\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    ")\n",
    "\n",
    "if len(model._multiple_gpus) > 1:\n",
    "    model._network = nn.DataParallel(model._network, model._multiple_gpus)\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of datalaoder:\", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._cur_task = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will only focus on the first stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import logging\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def tensor2numpy(x):\n",
    "    return x.cpu().data.numpy() if x.is_cuda else x.data.numpy()\n",
    "\n",
    "init_epoch = 200\n",
    "init_lr = 0.1\n",
    "init_milestones = [60, 120, 170]\n",
    "init_lr_decay = 0.1\n",
    "init_weight_decay = 0.0005\n",
    "\n",
    "epochs = 80\n",
    "lrate = 0.1\n",
    "milestones = [40, 70]\n",
    "lrate_decay = 0.1\n",
    "weight_decay = 2e-4\n",
    "\n",
    "\n",
    "model._network.to(model._device)\n",
    "if model._cur_task == 0:\n",
    "    # setup optimizer and scheduler\n",
    "    optimizer = optim.SGD(\n",
    "        model._network.parameters(),\n",
    "        momentum=0.9,\n",
    "        lr=init_lr,\n",
    "        weight_decay=init_weight_decay,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=init_epoch,\n",
    "    )  \n",
    "\n",
    "    # prog_bar = tqdm(range(init_epoch))\n",
    "    prog_bar = tqdm(range(20))\n",
    "    for _, epoch in enumerate(prog_bar):\n",
    "        model._network.train()\n",
    "        losses = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for i, (_, inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(model._device), targets.to(model._device)\n",
    "        #     logits = model._network(inputs)[\"logits\"]\n",
    "\n",
    "        #     loss = F.cross_entropy(logits, targets)\n",
    "        #     optimizer.zero_grad()\n",
    "        #     loss.backward()\n",
    "        #     optimizer.step()\n",
    "        #     losses += loss.item()\n",
    "\n",
    "        #     _, preds = torch.max(logits, dim=1)\n",
    "        #     correct += preds.eq(targets.expand_as(preds)).cpu().sum()\n",
    "        #     total += len(targets)\n",
    "\n",
    "        # scheduler.step()\n",
    "        # train_acc = np.around(tensor2numpy(correct) * 100 / total, decimals=2)\n",
    "\n",
    "        # if epoch % 5 == 0:\n",
    "        #     info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}\".format(\n",
    "        #         model._cur_task,\n",
    "        #         epoch + 1,\n",
    "        #         init_epoch,\n",
    "        #         losses / len(train_loader),\n",
    "        #         train_acc,\n",
    "        #     )\n",
    "        # else:\n",
    "        #     test_acc = model._compute_accuracy(model._network, test_loader)\n",
    "        #     info = \"Task {}, Epoch {}/{} => Loss {:.3f}, Train_accy {:.2f}, Test_accy {:.2f}\".format(\n",
    "        #         model._cur_task,\n",
    "        #         epoch + 1,\n",
    "        #         init_epoch,\n",
    "        #         losses / len(train_loader),\n",
    "        #         train_acc,\n",
    "        #         test_acc,\n",
    "        #     )\n",
    "        # prog_bar.set_description(info)\n",
    "\n",
    "    # logging.info(info)\n",
    "\n",
    "    # # save checkpoint for fair comparison & save runing time\n",
    "    # test_acc = model._compute_accuracy(model._network, test_loader)\n",
    "    # model.save_checkpoint(test_acc)\n",
    "    # logging.info(\"Save checkpoint successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model._multiple_gpus) > 1:\n",
    "    model._network = model._network.module"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
