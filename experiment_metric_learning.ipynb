{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿How well does a metric learning approach do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "seed = 1993\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_dir = \"/home/studio-lab-user/CIL_Survey/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset_gpu = {}\n",
    "eval_dataset_gpu = {}\n",
    "\n",
    "# dataset\n",
    "train = torchvision.datasets.CIFAR100(root=data_dir, download=True, transform=transforms.ToTensor())\n",
    "eval = torchvision.datasets.CIFAR100(root=data_dir, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# move dataset to gpu\n",
    "train_dataset_gpu_loader = torch.utils.data.DataLoader(train, batch_size=len(train), drop_last=True,\n",
    "                                            shuffle=True, num_workers=2, persistent_workers=False)\n",
    "eval_dataset_gpu_loader = torch.utils.data.DataLoader(eval, batch_size=len(eval), drop_last=True,\n",
    "                                            shuffle=False, num_workers=1, persistent_workers=False)\n",
    "train_dataset_gpu['images'], train_dataset_gpu['targets'] = [item.to(device=\"cuda\", non_blocking=True) for item in next(iter(train_dataset_gpu_loader))]\n",
    "eval_dataset_gpu['images'],  eval_dataset_gpu['targets']  = [item.to(device=\"cuda\", non_blocking=True) for item in next(iter(eval_dataset_gpu_loader)) ]\n",
    "\n",
    "# # normalize images\n",
    "# train_cifar_std, train_cifar_mean = torch.std_mean(train_dataset_gpu['images'], dim=(0, 2, 3)) \n",
    "# print(f\"Mean: {[f'{x:.4f}' for x in train_cifar_mean.tolist()]}\")\n",
    "# print(f\"Std: {[f'{x:.4f}' for x in train_cifar_std.tolist()]}\")\n",
    "# def batch_normalize_images(input_images, mean, std):\n",
    "#     return (input_images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n",
    "# batch_normalize_images = partial(batch_normalize_images, mean=train_cifar_mean, std=train_cifar_std)\n",
    "# train_dataset_gpu['images'] = batch_normalize_images(train_dataset_gpu['images'])\n",
    "# eval_dataset_gpu['images']  = batch_normalize_images(eval_dataset_gpu['images'])\n",
    "\n",
    "data = {\n",
    "        'train': train_dataset_gpu,\n",
    "        'eval': eval_dataset_gpu\n",
    "    }\n",
    "\n",
    "# pad images for later random cropping\n",
    "pad_amount = 4\n",
    "data['train']['images'] = F.pad(data['train']['images'], (pad_amount,)*4, 'reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric based data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def old_get_batches(data_dict, key, batchsize, indices=range(100), cutmix=False, cutmix_size=None):\n",
    "    # select subset of class indices \n",
    "    if indices is not None:\n",
    "        indices = torch.tensor(indices, device=device)\n",
    "        images, targets = data_dict[key][\"images\"], data_dict[key][\"targets\"] \n",
    "        samples = torch.isin(targets, indices)\n",
    "        images, targets = images[samples], targets[samples]\n",
    "        assert len(images) == len(targets)\n",
    "\n",
    "    num_epoch_examples = len(images)\n",
    "    shuffled = torch.randperm(num_epoch_examples, device=device)\n",
    "    crop_size = 32\n",
    "\n",
    "    ## Here, we prep the dataset by applying all data augmentations in batches ahead of time before each epoch, then we return an iterator below\n",
    "    ## that iterates in chunks over with a random derangement (i.e. shuffled indices) of the individual examples. So we get perfectly-shuffled\n",
    "    ## batches (which skip the last batch if it's not a full batch), but everything seems to be (and hopefully is! :D) properly shuffled. :)\n",
    "    if key == 'train':\n",
    "        images = batch_crop(images, crop_size) # TODO: hardcoded image size for now?\n",
    "        images = batch_flip_lr(images)\n",
    "        if cutmix:\n",
    "            images, targets = batch_cutmix(images, targets, patch_size=cutmix_size)\n",
    "\n",
    "    # # Send the images to an (in beta) channels_last to help improve tensor core occupancy (and reduce NCHW <-> NHWC thrash) during training\n",
    "    # images = images.to(memory_format=torch.channels_last)\n",
    "    for idx in range(num_epoch_examples // batchsize):\n",
    "        if not (idx+1)*batchsize > num_epoch_examples: ## Use the shuffled randperm to assemble individual items into a minibatch\n",
    "            yield images.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]), \\\n",
    "                  targets.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]) ## Each item is only used/accessed by the network once per epoch. :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from batch_transforms import batch_crop, batch_flip_lr\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batches(data_dict, key, batchsize, indices=range(100)):\n",
    "    # select subset of class indices \n",
    "    indices = torch.tensor(indices, device=device)\n",
    "    images, targets = data_dict[key][\"images\"], data_dict[key][\"targets\"] \n",
    "    samples = torch.isin(targets, indices)\n",
    "    images, targets = images[samples], targets[samples]\n",
    "    \n",
    "    assert len(images) == len(targets)\n",
    "\n",
    "    # as we are going to pair up the images, we need the size of the dataset to be even\n",
    "    if len(images) % 2 != 0:\n",
    "        images = images[:-1]\n",
    "        targets = targets[:-1]\n",
    "\n",
    "    num_epoch_examples = len(images)\n",
    "    shuffled = torch.randperm(num_epoch_examples, device=device)\n",
    "    crop_size = 32\n",
    "\n",
    "    images = images[shuffled]\n",
    "    targets = targets[shuffled]\n",
    "\n",
    "    if key == 'train':\n",
    "        images = batch_crop(images, crop_size)\n",
    "        images = batch_flip_lr(images)\n",
    "\n",
    "    # pair up the dataset\n",
    "    targets = targets.reshape(num_epoch_examples // 2, 2)\n",
    "    pairs = torch.eq(targets[:,0], targets[:,1])\n",
    "    # TODO: this takes much longer than before \n",
    "    # without 50% -> 30 ms\n",
    "    # with 50% -> 1 seg\n",
    "    # we need that roughly 50% of the pairs are positive and negative\n",
    "    if pairs.float().mean() < 0.5:\n",
    "        # get negative pairs\n",
    "        targets = targets.reshape(num_epoch_examples)\n",
    "        pairs = torch.stack([pairs, pairs], 1).reshape(num_epoch_examples)\n",
    "        neg = pairs == False\n",
    "        # permute them \n",
    "        perm = torch.randperm(len(pairs[neg]))\n",
    "        \n",
    "        images[neg] = images[neg][perm]\n",
    "        targets[neg] = targets[neg][perm]\n",
    "        targets = targets.reshape(num_epoch_examples // 2, 2)\n",
    "        pairs = torch.eq(targets[:,0], targets[:,1])\n",
    "    \n",
    "    images = images.reshape(num_epoch_examples // 2, 2, 3, images.shape[-1], images.shape[-2])\n",
    "    num_epoch_examples = len(images)\n",
    "\n",
    "    for idx in range(num_epoch_examples // batchsize):\n",
    "        yield images[idx*batchsize: (idx+1)*batchsize], targets[idx*batchsize: (idx+1)*batchsize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿How do I make sure that the implementation is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 93.1 ms, sys: 32.4 ms, total: 125 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for x, y in get_batches(data, \"train\", 128):\n",
    "    y\n",
    "    \n",
    "torch.eq(y[:,0], y[:,1]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 16.3 ms, total: 37 ms\n",
      "Wall time: 35.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "for x, y in old_get_batches(data, \"train\", 128):\n",
    "    y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "# images_pairs = data[\"train\"][\"images\"].reshape(25000, 2, 3, 40, 40)\n",
    "# labels_pairs  = data[\"train\"][\"targets\"].reshape(25000, 2)\n",
    "\n",
    "# # data[\"train\"][\"images\"][1].shape\n",
    "# to_pil_image(data[\"train\"][\"images\"][0]).show()\n",
    "# print(data[\"train\"][\"targets\"][0])\n",
    "# to_pil_image(data[\"train\"][\"images\"][1]).show()\n",
    "# print(data[\"train\"][\"targets\"][1])\n",
    "\n",
    "# to_pil_image(images_pairs[0][0]).show()\n",
    "# to_pil_image(images_pairs[0][1]).show()\n",
    "# print(labels_pairs[0][0])\n",
    "# print(labels_pairs[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental trainining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch:Python",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
