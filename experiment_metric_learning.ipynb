{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿How well does a metric learning approach do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "seed = 1993\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Mean: ['0.5071', '0.4865', '0.4409']\n",
      "Std: ['0.2673', '0.2564', '0.2762']\n"
     ]
    }
   ],
   "source": [
    "train_dataset_gpu = {}\n",
    "eval_dataset_gpu = {}\n",
    "\n",
    "# dataset\n",
    "train = torchvision.datasets.CIFAR100(root=data_dir, download=True, transform=transforms.ToTensor())\n",
    "eval = torchvision.datasets.CIFAR100(root=data_dir, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# move dataset to gpu\n",
    "train_dataset_gpu_loader = torch.utils.data.DataLoader(train, batch_size=len(train), drop_last=True,\n",
    "                                            shuffle=True, num_workers=2, persistent_workers=False)\n",
    "eval_dataset_gpu_loader = torch.utils.data.DataLoader(eval, batch_size=len(eval), drop_last=True,\n",
    "                                            shuffle=False, num_workers=1, persistent_workers=False)\n",
    "train_dataset_gpu['images'], train_dataset_gpu['targets'] = [item.to(device=\"cuda\", non_blocking=True) for item in next(iter(train_dataset_gpu_loader))]\n",
    "eval_dataset_gpu['images'],  eval_dataset_gpu['targets']  = [item.to(device=\"cuda\", non_blocking=True) for item in next(iter(eval_dataset_gpu_loader)) ]\n",
    "\n",
    "# normalize images\n",
    "train_cifar_std, train_cifar_mean = torch.std_mean(train_dataset_gpu['images'], dim=(0, 2, 3)) \n",
    "print(f\"Mean: {[f'{x:.4f}' for x in train_cifar_mean.tolist()]}\")\n",
    "print(f\"Std: {[f'{x:.4f}' for x in train_cifar_std.tolist()]}\")\n",
    "def batch_normalize_images(input_images, mean, std):\n",
    "    return (input_images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n",
    "batch_normalize_images = partial(batch_normalize_images, mean=train_cifar_mean, std=train_cifar_std)\n",
    "train_dataset_gpu['images'] = batch_normalize_images(train_dataset_gpu['images'])\n",
    "eval_dataset_gpu['images']  = batch_normalize_images(eval_dataset_gpu['images'])\n",
    "\n",
    "data = {\n",
    "        'train': train_dataset_gpu,\n",
    "        'eval': eval_dataset_gpu\n",
    "    }\n",
    "\n",
    "# pad images for later random cropping\n",
    "pad_amount = 4\n",
    "data['train']['images'] = F.pad(data['train']['images'], (pad_amount,)*4, 'reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric based data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from batch_transforms import batch_crop, batch_flip_lr\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_batches(data_dict, key, batchsize, paired=True, indices=range(100)):\n",
    "    # select subset of class indices \n",
    "    indices = torch.tensor(indices, device=device)\n",
    "    images, targets = data_dict[key][\"images\"], data_dict[key][\"targets\"] \n",
    "    samples = torch.isin(targets, indices)\n",
    "    images, targets = images[samples], targets[samples]\n",
    "    \n",
    "    assert len(images) == len(targets)\n",
    "\n",
    "    num_epoch_examples = len(images)\n",
    "    shuffled = torch.randperm(num_epoch_examples, device=device)\n",
    "    crop_size = 32\n",
    "\n",
    "    # shuffle the dataset\n",
    "    images = images[shuffled]\n",
    "    targets = targets[shuffled]\n",
    "\n",
    "    # transforms\n",
    "    if key == 'train':\n",
    "        images = batch_crop(images, crop_size)\n",
    "        images = batch_flip_lr(images)\n",
    "\n",
    "    if paired: \n",
    "        # as we are going to pair up the images, we need the size of the dataset to be even\n",
    "        if len(images) % 2 != 0:\n",
    "            images = images[:-1]\n",
    "            targets = targets[:-1]\n",
    "        # pair up the dataset targets = targets.reshape(num_epoch_examples // 2, 2)\n",
    "        binary_targets = torch.eq(targets[:,0], targets[:,1])\n",
    "        # we need that roughly 50% of the pairs are positive and negative\n",
    "        while binary_targets.float().mean() < 0.5:\n",
    "            # unpair the target and binary_targets\n",
    "            targets = targets.reshape(num_epoch_examples)  \n",
    "            binary_targets = torch.stack([binary_targets, binary_targets], 1).reshape(num_epoch_examples)\n",
    "            # get negative elements from negative pairs\n",
    "            neg = binary_targets == False\n",
    "            # permute them hoping some of them turn into positive pairs\n",
    "            perm = torch.randperm(len(binary_targets[neg]))\n",
    "            images[neg] = images[neg][perm]\n",
    "            targets[neg] = targets[neg][perm]\n",
    "            # re-pair the targets\n",
    "            targets = targets.reshape(num_epoch_examples // 2, 2)\n",
    "            binary_targets = torch.eq(targets[:,0], targets[:,1])\n",
    "        ## TODO: ensuring 50% distribtution takes much longer than before \n",
    "        ## without 50% -> 30 ms\n",
    "        ## with 50% -> 1 seg\n",
    "        \n",
    "        images = images.reshape(num_epoch_examples // 2, 2, 3, images.shape[-1], images.shape[-2])\n",
    "        num_epoch_examples = len(images)\n",
    "\n",
    "    for idx in range(num_epoch_examples // batchsize):\n",
    "        if paired:\n",
    "            yield images[idx*batchsize: (idx+1)*batchsize, 0], images[idx*batchsize: (idx+1)*batchsize, 1],  binary_targets[idx*batchsize: (idx+1)*batchsize].int()\n",
    "        else: \n",
    "            yield images[idx*batchsize: (idx+1)*batchsize], targets[idx*batchsize: (idx+1)*batchsize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿How do I make sure that the implementation is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¿Are there any duplicated or overlapping pairs?\n",
    "    I think there are not since the pairs are selected by reshaping the original 1d-tensor of targets in a matrix with 2 rows, so if there were not any duplicates in the original tensor, then there must not be any duplicates in the pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "for x1, x2, y in get_batches(data, \"train\", 128):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "idx_to_class = {i:c for c,i in train.class_to_idx.items()}\n",
    "\n",
    "print(idx)\n",
    "display(to_pil_image(x1[idx]))\n",
    "display(to_pil_image(x2[idx]))\n",
    "\n",
    "print(y[idx])\n",
    "# print([idx_to_class[i] for i in y[idx].tolist()])\n",
    "idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "for x, y in old_get_batches(data, \"train\", 128):\n",
    "    y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "n = len(data[\"train\"][\"images\"])\n",
    "\n",
    "images_pairs = data[\"train\"][\"images\"].reshape(n // 2, 2, 3, 40, 40)\n",
    "labels_pairs = data[\"train\"][\"targets\"].reshape(n // 2, 2)\n",
    "\n",
    "# show first and second image\n",
    "display(to_pil_image(data[\"train\"][\"images\"][0]))\n",
    "print(data[\"train\"][\"targets\"][0])\n",
    "display(to_pil_image(data[\"train\"][\"images\"][1]))\n",
    "print(data[\"train\"][\"targets\"][1])\n",
    "\n",
    "# show first pair\n",
    "display(to_pil_image(images_pairs[0][0]))\n",
    "display(to_pil_image(images_pairs[0][1]))\n",
    "print(labels_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y = next(get_batches(data, \"train\", 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = resnet18()\n",
    "        self.ndim = self.conv.fc.in_features\n",
    "        self.conv = nn.Sequential(*list(resnet18().children())[:-1])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.ndim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "     \n",
    "    def forward_once(self, x):\n",
    "        x = self.conv(x)         # (bs, ndim, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # (bs, ndim)\n",
    "        x = self.fc(x)           # (bs, 2)\n",
    "        return x \n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)  # (bs, 2)\n",
    "        output2 = self.forward_once(input2)  # (bs, 2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork()\n",
    "net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2]) torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "out1, out2 = net(x1, x2)\n",
    "print(out1.shape, out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Mechanism\n",
    "\n",
    "f() -> (x, y)\n",
    "\n",
    "1. Calculate mean embedding of each class in the dataset.\n",
    "2. x -> d(x, mu_i) -> min d(x, mu_i) 1-NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Contrastive Loss Function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "      # Calculate the euclidean distance and calculate the contrastive loss\n",
    "      euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
    "      loss_contrastive = torch.mean((1-label) * euclidean_distance**2 + (label) * torch.clamp(self.margin - euclidean_distance, min=0.0)**2)\n",
    "\n",
    "\n",
    "      return loss_contrastive    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ContrastiveLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batch_transforms import get_batches\n",
    "\n",
    "def train(optimizer, task):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    indices = range(increment * task, increment * (task + 1))\n",
    "    for batch_idx, (inputs1, inputs2, targets) in enumerate(get_batches(data, \"train\", batch_size, indices=indices)):\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = net(inputs1, inputs2)\n",
    "        loss = criterion(outputs1, outputs2, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/(batch_idx + 1)\n",
    "\n",
    "def eval(from_task, to_task, emb):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    indices = range(from_task * increment, (to_task+1) * increment)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs1, inputs2, targets) in enumerate(get_batches(data, \"eval\", batch_size, indices=indices)):\n",
    "            outputs1, outputs2 = net(inputs1, inputs2)\n",
    "            loss = criterion(outputs1, outputs2, targets)\n",
    "            test_loss += loss.item()\n",
    "        return test_loss/(batch_idx + 1)\n",
    "\n",
    "def mean_embeddings(net, task):\n",
    "    mean_embds = torch.tensor(2, (task+1)*increment)\n",
    "    for idx in range((task+1)*increment):\n",
    "        x, _ = next(get_batches(data, \"train\", 500, paired=False, indices=range(idx)))\n",
    "        print(x.shape)  # (500, 3, 40, 40)\n",
    "        emb = net(x) # (500, 2)\n",
    "        mean_emb = emb.mean(0)\n",
    "        mean_embds[idx] = mean_emb\n",
    "    return mean_embds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_train(task, to_task)\n",
    "    # select optimize and scheduler\n",
    "    if task == 0:\n",
    "        epochs = init_epochs\n",
    "        optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=init_lr, weight_decay=init_weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    else: \n",
    "        epochs = rest_epochs\n",
    "        optimizer = optim.SGD(net.parameters(), momentum=0.9, lr=rest_lr, weight_decay=rest_weight_decay)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=rest_milestones, gamma=rest_lr_decay)\n",
    "\n",
    "    pbar = tqdm(range(epochs), unit=\"epoch\")\n",
    "    for epoch in pbar:\n",
    "        tloss, tacc = train(optimizer, task)\n",
    "        eloss, eacc = eval(from_task=0, to_task=to_task)\n",
    "        scheduler.step()\n",
    "        pbar.set_postfix({\"Train Loss\": tloss, \"Train Acc\": tacc, \"Eval Loss\": eloss, \"Eval Acc\": eacc})\n",
    "\n",
    "    return eloss, eacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the embedding with t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
